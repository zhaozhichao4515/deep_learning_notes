## 权重初始化

**1. 错误：全零初始化。**

如果对权重进行全零初始化，那么网络中的每个神经元都计算出同样的输出，然后它们就会在反向传播中计算出同样的梯度，从而进行同样的参数更新。换句话说，如果权重被初始化为同样的值，会产生对称性。设想你在爬山，但身处直线形的山谷中，两边是对称的山峰。由于对称性，你所在之处的梯度只能沿着山谷的方向，不会指向山峰；你走了一步之后，情况依然不变。结果就是你只能收敛到山谷中的一个极大值，而走不到山峰上去。

tensorflow 提供如下API 用于进行全零或者全一的初始化：

`tf.zeros(shape, dtype=tf.float32, name=None)`

`tf.ones(shape, dtype=tf.float32, name=None)`

**2. 均匀分布**

因为全零初始化会导致对称性。因此，权重初始值要非常接近0又不能等于0。解决方法就是将权重初始化为很小的数值，以此来打破对称性。可以使用均匀分布的随机数。tensorflow 提供的如下接口用于生成均匀分布：

`tf.random_uniform(shape, minval=0, maxval=None, dtype=tf.float32, seed=None, name=None)`

其中`minval`, `maxval` 的常见取值为：`-1/sqrt(x)`和`1/sqrt(x)`, 其中x为输入神经元的个数。

*注意：*并不是小数值一定会得到好的结果。例如，一个神经网络的层中的权重值很小，那么在反向传播的时候就会计算出非常小的梯度（因为梯度与权重值是成比例的）。这就会很大程度上减小反向传播中的“梯度信号”，在深度网络中，就会出现问题。

**3. 高斯分布**

一个更好的初始化方式为高斯分布，每个神经元的权重向量都被初始化为一个随机向量，而这些随机向量又服从一个多变量高斯分布，这样在输入空间中，所有的神经元的指向是随机的。

Python 的原生实现：`w = np.random.randn(n)`

Tensorflow 提供的API 为：`tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)`



我们使用`1/sqrt(n)`校准方差来解决随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也增大的问题。我们可以除以输入数据量的平方根来调整其数值范围，这样神经元输出的方差就归一化到1了。

Python 原生实现：`w = np.random.randn(n) / sqrt(n)`

Tensorflow 实现：`tf.random_normal([n])`

两者是等价的，详细证明见CS231n。其中n为输入数据的数量。这样就保证了网络中所有神经元起始时有近似同样的输出分布。实践经验证明，这样做可以提高收敛的速度。



Tensorflow 还提供了 `tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)](https://www.tensorflow.org/api_docs/python/tf/truncated_normal)`

来生成truncated_normal 来进行权重初始化。



**4. Xavier**

Glorot等在论文[Understanding the difficulty of training deep feedforward neural networks](https://link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)中推荐初始化公式为`Var(w) = 2 / (n_in+n_out)`，其中`n_in`和`n_out`是前一层和后一层中单元的个数。



**5. MSRA**

该主题下最新的一篇论文是：[Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://link.zhihu.com/?target=http%3A//arxiv-web3.library.cornell.edu/abs/1502.01852)，He等人在论文中针对ReLU神经元的提出了一种特殊初始化，并给出结论：网络中神经元的方差应该是`2.0/n` ,这个形式是神经网络算法使用ReLU神经元时的当前最佳推荐。

原生Python 代码实现：`W = np.random.randn(n)*sqrt(2.0/n)`

使用 Tensorflow 实现：`    tf.Variable(tf.random_normal(layer_1_weight_shape, stddev= 2.0/n))` , 其中`n`为权重相连的神经元的个数。对于输入层，由于其没有应用ReLu函数，所以其权重的房车设置为`1.0/n`。



**6. 稀疏初始化（Sparse initialization）。**另一个处理非标定方差的方法是将所有权重矩阵设为0，但是为了打破对称性，每个神经元都同下一层固定数目的神经元随机连接（其权重数值由一个小的高斯分布生成）。一个比较典型的连接数目是10个。

## 偏置的初始化

通常将偏置初始化为0，这是因为随机小数值权重矩阵已经打破了对称性。

## 实践

当前的推荐是使用ReLU激活函数，并且使用**w = np.random.randn(n) \* sqrt(2.0/n)**来进行权重初始化。



## 批量归一化（Batch Normalization）

[批量归一化](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1502.03167)是loffe和Szegedy最近才提出的方法，该方法减轻了如何合理初始化神经网络这个棘手问题带来的头痛：），其做法是让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。因为归一化是一个简单可求导的操作，所以上述思路是可行的。在实现层面，应用这个技巧通常意味着全连接层（或者是卷积层，后续会讲）与激活函数之间添加一个BatchNorm层。对于这个技巧本节不会展开讲，因为上面的参考文献中已经讲得很清楚了，需要知道的是在神经网络中使用批量归一化已经变得非常常见。在实践中，使用了批量归一化的网络对于不好的初始值有更强的鲁棒性。最后一句话总结：批量归一化可以理解为在网络的每一层之前都做预处理，只是这种操作以另一种方式与网络集成在了一起。